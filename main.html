<head>
  <title>Algorithms Aren't The Solution</title>
  <style>
    .main {
      width: 60%;
      margin: 0 auto;
    }
    #title {
      width: 70%;
    }
    .body-text {
      float: right;
    }
  </style>
</head>
<body>
  <div class="main">
    <center>
    <h1 id="title">Algorithms Aren't The Solution To A 
      Broken Criminal Sentencing System, At Least Not Yet</h1>
      <div style="font-size: 20px;">
        <strong><p><i>By: Nick Karlovich</i></p></strong>
      </div>
    </center>
    <img width="1000px" height="800px" src=".\images\firstdraft-cover.png"/>
    <div>
        <p class="body-text">
          A major problem within the U.S. Criminal Justice System is the inconsistency
          and unpredicatability of judges and their decisions on what the correct amount
          of punishment should be for any given crime.  There are a countless
          number of news stories showcasing how a criminal will get an extremely leniant
          punishment, then two days later, a similar case results in the defendant recieving a 
          punishment that is so harsh in comparison to the previous case you have to wonder if the
          court system is just picking punishments at random.
        </p>
        <p> For example, here are four cases you may recognize where the punishments don't match the crime committed.</p>
        <img src=".\images\small-four-array.png"/>
        <details>
          <summary>
          Racial info
          </summary>
          ( [in progress thoughts] [don't read this, maybe just get the vibe for now]
          Proponents of racial equiality groups will typically point to specific cases
          where guilty white defendends will be given a lesser punishment for a crime,
          whereas a similar guilty black defendent will be given a harsher punishment
          for the same crime.  Personally, I do believe that this is an issue, but I
          am not making this argument here as it cannot be shown and is outside the scope 
          of this visualization.
        
          The statement that it is statistically correlated that your race has an independent affect
          on the severity of a criminal punishment is a very bold statement, and one I will
          not attempt to show here.  This fact is just very difficult to prove
          due to there being so many variables.  There are not enough cases where enough of these
          variables line up to draw any meaningful statistical conclusions (Judge, Race, Age,
          Previous Criminal History, Crime, Location, Time, Guilt, Remorse, Intent, etc.))
        
         A major problem within the U.S. Criminal Justice System is the existence
        of systematic bias in the sentencing of criminals of different races.  Now this can't
        be proved mathematically or with any true statistical signifigance.  
        
        Black
         individuals are more likely to be get longer prison sentences and be 
         considered "higher risk" compared to white individuals with similar criminal
          histories.  This was proven 
      </details>
        <p class="body-text">
          But this percieved inconsistency reaches beyond our guesswork and the stories we
          read about it the news. It has 
          actually been shown in studies how inconsistent and unpredicatable
          sentencings can be across different judges.  In a study involving 47 virgina 
          district attorneys, each judge was given 
          5 fabricated cases and were told to determine the guilt of the defendant
          and if found guilty, determine the appropriate sentence.
          
          Here are the results of two of those cases as examples:
        </p>
        <h3> Case A: </h3>
        <p class="body-text">
          <strong>Description:</strong>
          An eighteen year old female defendant who was apprehended for possession
of marijuana. She was arrested with her boyfriend and seven other acquaintances. Evidence
of a substantial amount of smoked and unsmoked marijuana was found; however, no
marijuana was discovered directly on the defendant's person. She had no previous criminal 
record, was a good student from a middle class home and was neither rebellious nor apologetic
for her actions.
        </p>
        <img src="./images/Case 1.png"/>
        <!--
          twenty-nine voted not guilty (61.7%)
          eighteen voted guilty (38.3%)
          - 8 (44.4%) probation
          - 4 (22.2%) fine
          - 3 (16.7%) probation & fine
          - 3 (16.7%) jail term
          
          -> should above percentages be of total likelyhood or likelyhood of
          if you are already found guilty.
        -->
        <p>
          so, depending on who you got as your judge you had a
          <ul>
            <li>61.7% chance of having a clean record, </li>
            <li>17% chance of going on probation</li>
            <li>8.5% chance of getting a fine</li>
            <li>6.3% chance of getting probation and fine</li>
            <li>6.3% chance of going to jail</li>
          </ul>
          
          The point to note here, is depending on your judge, some may find you 
          innocent while other judges will not only find you guilty, but believe
          that the appropriate punishment is jail time.  Regardless of what the
          proper outcome should be, the discrepencies and variation between the
          possible punishments is extreme.
        </p>
          
          
          
        <h3> Case B: </h3>
        <p class="body-text">
          <strong>Description:</strong>
          This case was a burglary. Residents of a
household returned home to find their television set missing, and, hearing a noise, noticed
two persons carrying a large object down the
street. Soon afterwards, officers arrested a
forty-six year old man and his son, who were
pawning the TV set, which was identified by
its serial numbers. The father, whose fingerprints matched those found in the house, had
four previous convictions for petty larceny, one
for reckless driving and one for assaulting a
police officer. The son had only one traffic
conviction and claimed his father had told him
a friend gave him the television set.

Due to how this case is worded only 31 of the judges gave a sentencing for the father
(the other 16 certified to grand jury) [figure out what the hell this means]
</p>
<img src="./images/Case 2.png"/>
            <!--
of those 31 who gave sentences, all found the father guilty, 26 of those decided that
the father deserved jail time, and 13 of them also included a fine. (the other 5 gave
different sentences)
    -->
    <p>
      The point to note here is punishment.  While all the judges agreed that
      the father was guilty, the punishments given by the judges have an astionishing
      amount of variability.
      
      At least one judge suggested that the father should face 1 month of jail time
      and a $100 fine for his crime, and on the opposite side of the spectrum, at least
      one other judge handed down a sentence of 5 years in jail.  That is a
      disparity between punishments by over 60x (1 month vs 12 months in a year * 5 years)
    </p>
        
        
        <!--<p class="body-text">
Within the same case, there were also conflicting decisions on the son's
guilt.

With 27 of the 47 (57.4%) finding the son guilty, while the other twenty (42.6%) found
the son innocent. Even amongst the judges who found the son guilty, some
advised for full jail terms, while others suggested suspended jail terms (no jail time
contingent on something, typically community service or keeping a clean record)
      </p> -->
      <p class="body-text">
These two fictional cases highlight the main issue many people see with the 
justice system, it is impossible for our judges to view cases in the exact same way, 
resulting in these incredible discrepencies where based on your judge, you could
walk free, or be sentenced to jail. And even when the judges agree that you deserve
jailtime  the amount 
of time for any given crime could vary from 1 month up to 60x that amount at 5 years.
      </p>
      <p class="body-text">
So, there's an issue between judges, each judge has their opinions and ideas which
will influence how they decide on cases.  These differences in opinion are impossible 
to remove from the court system without removing a human judge from the equation entirely 
and I'm not quite sure we'd want to remove the human element anyways.  That's part of
what a court is, the idea going back to the Magna Carta in 1297, that a courtroom should
consist as a jury of one's own peers, not one person reading word-for-word the laws from a book.
So, removing the humans from the courtroom entirely is out of the question and 
due to human nature it is likely that we will never have variation in sentencing,
  </p>
  <p> but at least judges make consistent decisions in the cases they do see ...
<!--Unless there were some way to keep the human element while making decisions across judges more consistent. -->
    </p>
      <p>
      <strong><i>
        But, what if judges aren't even consistent against themselves?
      </i></strong>
      </p>
        <p class="body-text">
          In a more recent study 81 UK judges were asked to give bail to 
          41 fictious cases. [hello world]  Unbeknownst to the judges, hidden amongst 
          the 41 cases were 7 cases that
          were duplicates of earlier cases with only the names changed.  
          Most judges didn't manage to make the same decision on the same case when 
          seeing it for the second time.  Astonishingly, some judges did no better
          than matching their own answers than if they were, quite literally, awarding
          bail at random.[direct quote Hello World]
        </p>



<!--<<maybe not include>>
          In real world cases
           other seemingly random events have been shown to
          have correlation with the outcomes of cases
          
          
          <<<[hello world, pg 75]
          Although why exactly this happens can't be pinned down, in her book Hello World, Hannah Fry 
          begins to describe how many factors play into how human judges make their
          decisions, "Judges with daughters are more likely to make decisions more favoriable to women[1], 
          judges are less likely to award bail if the local sports team has lost recently. [One] famous study 
          even suggested that the time of day affects your chances of a favorable outcome[6]...
          Another famous study showed that an individual judge will avoid making too many similar judgements
          in a row.  Your chances therefore of being award bail drop off a cliff if four successful cases
          were heard immediately before yours[64]"
-->      
<br>
          <p>
          There's disagreement when deciding the guilt
          of a case amongst different judges, and when the judges agree on the guilt,
          there's extreme disagreements about how severe the punishments should be.
          Even against themselves, judges have shown that more often than not, they will
          not make the same decision on two duplicate cases.
          So, now it's understandable why there is a push in society today
          to make the courts more fair.  With this being the age of the computer;
          Algorithms, machine learning, and AI are being explored as possible soultions
          to the problem of inconsistent sentencing. But before we get to that topic, lets look at 
          how algorithms have been used in other fields to fix problems that humans couldn't do alone.
          </p>
          
    </div>
    <!-- News articles of people getting criminally long sentences when not needed -->
    <!-- <img src="images/black-vs-white-compas-scores.png">[1]</img>
    <p>
      The exact reasoning behind why this is cannot be determined, but there are 
      possible reasons as to why human judges don't perform well when trying to fairly
      give punishments.
    </p>
    
    <ol type="1">
      <li>Biases</li>
      <ul>
        <li> Humans are known to be biased and it is often difficult for us
          to not let those influence our decisions [cite] </li>
      </ul>
      <li>Corruption</li>
      <ul>
        <li>Judges have the possibility to be bribed, convied, threatened into
           making decisions in favor of certain parties. 
            <p>
             [FOOTNOTE 1: it is also possible for algorithms to be modified, which
             is why a good algorithm would be publicly available to everybody (which 
             I suggest), then the argument could be made that you could compromise 
             the local integrity of a machine making these decisions, for which a
             counter argument could be made about all judicial decisions requiring
             a hash in order to validate results, for which a third counter argument
             could be made that you could give false hashes and you can probably see
             how this argument quickly spirals into a never ending circle of who to 
             trust which is too deep of a concept to cover in this visualization but
             I would suggest you check out [resource talking about security of
             download hashes] ]
             </p>
        </li>
      </ul>
      <li>Consistency & Fairness</li>
      <ul>
        <li>It has been shown that when judges are given the the same legal case
          that they will not consistently come to an agreement on what punishment
          is fair.  In this study [hello world] on a misdemeanor drug charge depending
          on what judge saw your case you had the chance to walk away with no jail time
          or could be put into jail for over 3 years, based purely on the judge's
          individual opinion
        </li>
        <li>Even more shockingly, judges don't make consistent decisions with themselves.
          In a study [hello world] done in ...., judges were given 60 cases, 7 of which
          were duplicates with the names changed and judges still did not give the cases
          the same sentencing, even though the details of the case were exactly the same.
        </li>
      </ul>
    </ol>
    <p>
      Given these glaring issues with judges it is no wonder why we've been looking 
      for more consistent alternatives to human judges.[footnote 2, I don't have
       anything against judges, they do their best and I appreciate that, it's only
       human nature that we're so inconsistent]  Recently, the U.S. Government
      has turned towards using algorithms as a possible solution.
    </p>    



    <p>
      So, before we get into algorithms for use in the courtroom. Lets take a look
      at how other fields of study have integrated algorithms into their work.
    </p>
--> 
  <div>
    <div style="width:50%; float:left;">
      <h2> Algorithms for detecting breast cancer </h2>
      <p>
        PathAI: [3]https://www.pathai.com/what-we-do/

        An algorithm that assists a pathologist's cancer diagnosis process by reducing 
        the amount of time each diagnosis took.  The algorithm's introduction produced


        doctors are about 96% accurate in detecting cancers
        pg 88, (missed 27% of smaller hidden instances of cancer), 
        Never had a (false positive)

        best of algorithms from (CAMELBAK16) was 92.4% accurate, but with 8 false positives

        Combining these two together though, results in a 99.5% accuracy rating, with no false positives.
      </p>
    </div>
    <div style="width:50%; height:300px; float:right;">
    <h2> Intelligent Path Creation Algorithms </h2>

    <p> An algorithm that helps delivery truck drivers take more efficient routes when 
      delivering packages.

      Most notably these algorithms figured out important patterns to traffic in cities
      and have allowed companies like USPS/UPS/FedEx to deliver more packages quicker and 
      more efficiently.
    </p>
    </div>
  </div>
  <div>
    <br>
    <h3>
      <strong> So now that we've seen the kinds of ways that algorithms can be used to 
        solve problems that humans can't do well, lets put together what our <i>ideal</i>
        algorithm would have
      </strong>
    </h3>

  <details>
    <summary>Important for later</summary>
      There is an important distinction between these algorithms, PATH AI and
      intelligent path creation. and the COMPAS algorithm.

      PathAI and intelligent path creation cannot run on its own.  They require 
      human interaction to run

      I fhte path AI went on its own, it would misdiagnose 8% of the people it scanned (not good, cite)

      if the intelligent path creation algorithm tried to run on its own, it
      would fail because it doesn't know the rules of the road, and couldn't react
      to rapidly changing road conditions.


      Compare this to the COMPAS algorithm.  It's new enough yet where the algorithm 
      isn't fully trusted on its own but that level of trust is rapidly approaching.

      The downsides to the other algorithms (people being misdiagnosed and car crashes)
      are much more visible to us than the downsides of the algorithm (racial biased
      steryotyping, etc.)  And why wouldn't judges want to use COMPAS to make their decisions
      for them.  Besides making their job easier, it gives the judges an easy get-out-of-jail free card
      if the algorithm misbehaves.  "Oh I didn't sentence that person to the death 
      penalty for jaywalking, it was the algorithm, it wasn't my fault.  The algorithm,
      is smarter than me so I trusted it..."

  </details>
</div>
<div>

    <h3> Features of our Algorithm that can outperform humans </h3>

    <ul>
      <li> Unbiased </li>
      <ul>
        <li>
          
        </li>
      </ul>
      <li> More accurate </li>
      <ul>
        <li> These algorithms can access the entire history of all legal cases ever passed through
          the court systems.  It can make infrences, detect patterns, and have more
          requisite knowledge than any judge could ever dream of.  Thus, this reuslts in
          an algorithm better at making decisions than a human judge could ever be due
          to it's ability to consider more variables and history when making its decisions.
        </li>
      </ul>
      <li> Can't be bribed or threatened </li>
      <ul>
        <li> A mathematical formula doesn't care about the politics of any situation,
          it has the capability to assign guilt and nothing else.
      </ul>
      <li> Consistent and Fair, Blind Justice </li>
      <ul>
        <li> Computers are very good at consistently following
          a formula or logical progression of steps.  Unlike judges, a computer's decision
          making isn't affected by the time of day, the local sports team's winning or losing
          streak, or the <i>narrow bracketing</i> cognitive bias [3].
        </li>
          [hello world pg75]
      </ul>
    </ul>
    
    [3] https://journals-sagepub-com.ezp2.lib.umn.edu/doi/pdf/10.1177/0956797612459762

    <<<[hello world, pg 75]
    Although why exactly this happens can't be pinned down, in her book Hello World, Hannah Fry 
    begins to describe how many factors play into how human judges make their
    decisions, "Judges with daughters are more likely to make decisions more favoriable to women[1], 
    judges are less likely to award bail if the local sports team has lost recently. [One] famous study 
    even suggested that the time of day affects your chances of a favorable outcome[6]...
    Another famous study showed that an individual judge will avoid making too many similar judgements
    in a row.  Your chances therefore of being award bail drop off a cliff if four successful cases
    were heard immediately before yours[64]"





    <h4> Goals: </h4>
    <ul>
      <li> Prevent future crimes </li>
      <li> Assign the minimal punishment that will prevent a criminal from re-offending </li>
    </ul>

    <h4> Not Goals: </h4>
    <ul>
      <li> Determine guilt of accused </li> MAYBE WE DON"T INCLUDE THIS POINT OR WE DON"T INCLUDE 
      EARLIER EXAMPLE, I really like Case 1, because it shows the discrepencies in determining guilt, but I also claim
      here that we shouldn't really be making our algorithm determine guilt, because that's a much more human thing
      and instead we should only be focused on the discrepencies in crime.
      
      Maybe earlier instead of saying "look at how judges can't agree on guiltiness",
      we do a yeah, it's an issue but an issue for a later time, -> Even amongst people who
      found her guilty, look at how big of discrepencies, some said just monetary fine, the others said JAIL
      <ul>
        <li> Our algorithm will assume that the person has been found guilty </li>
        <li> Determining guilt should be left up to judges and courts </li>
      </ul>
      <li> Minority Report / Person of Interest style of predicting crimes before they happen </li>
      <li> Stop first time offenders [note 3: argument could be made that punishments 
        by earlier court cases could be used as deterrent, but determining if certain 
        punishments influence future crimes is incredibly hard to determine (multivariate problems) 
        so for this visualization we won't be considering this possibility]
      </li>
    </ul>

    In order to better explain how our algorithm would work in theory the example of 
    Jim and Bill is given.

    Jim and his kid John are living below the poverty line and due to some unforseen
     medical expenses, Jim doesn't have enough money for groceries, so he goes to a 
     gas station and steals some rice and bread.  Jim normally wouldn't do this but
      given the option between starving his child and stealing, he's going to take 
      his chances.
      
    Bill isn't in an ideal place economically, he has a mortgage and a car loan etc, but
    he can definitely pay for his groceries and doesn't need to worry about going hungry.
    Bill steals a candy bar and soda from the same gas station as Jim because he just
    doesn't feel like spending the $3 to buy them.

    Jim and Bill both get caught and are found guilty of stealing.  Here is where our
    algorithm comes in and decides how much of what kind of punishment each person
    should get in order to disuade them from ever commiting another crime.
    [Note 4: Special note here, care is needed to MINIMIZE the time, without this
    clause, an algorithm might quickly learn that if for any crime committed, the optimal
     punishment is death.  The algorithm would have successfully created a solution
     where there is a 0% reoffend rate... at the cost of murdering every single
     person who ever commits a crime.]

    <img src="./images/basic-reoffend-table.png"/>

    The optimal solution to the cost-matrix above is for our algorithm to put Bill away for X 
    months and for Jim to get connected with some food-stamp / free meals program as 
    this will minimize the reoffend chances for both people.

    Obviously this example is very Black & White and the real world isn't as cut and dry 
    as this, but figuring that out should be the job of a large, complex, algorithm 
    that can detect patterns better than us humans can.

    So, this algorithm seems pretty great right?
    Well, if only this is how it actually worked.  Instead we have COMPAS.


    COMPAS is a "risk-assessment" algorithm developed by Equivant and is used by some U.S. courts
    to assess the likelyhood of a defendant re-offending.

    COMPAS has been under fire since its inception for its racially biased results (pro publica
    article here).  Although there has been counter research re-affirming that an algorithm
    can be created to show actuarial risk can be predicted free of racial and/or gender bias"
    (although this doesn't imply that COMPAS is one of those algorithms that does this.)

    Futher reserach has brought up that COMPAS software is only slightly more accurate than
    individuals with little or no criminal justice experience and is less accurate than
    when those individuals answers are pooled together (63% for individual, 65% for COMPAS, 
    67% for individuals grouped.)  Which aren't particularly inspiring statistics for
    an algorithm which is supposed [to replace judges in sentencing](I might be straw
    maning the opponent here, need to find a source for this)

    Looking at COMPAS and comparing it to the features that we looked for before,
    we can actually see how those features that we assumed earlier have actually
    come to make the algorithms we have now WORSE.

    <ul>
      <li> <strike>Unbiased</strike> </li>
      <ul>
        <li>
          This is only true if the training data that is provided to the algorithm is
          unbiased as well, and because COMPAS is not open-source, we don't know what
          data they used to train their algorithm (although it is likely that if they
          used previous court cases as their data, that the biases and racial disparities
          that exist in our system today was fed into the algorithm as the "norm")
        </li>
      </ul>
      <li> <strike>More accurate</strike> </li>
      <ul>
        <li>
          A "more accurate" algorithm allows for judges to fully rely on the algorithm
          even if the algorithm gives what would typically be considered an "unfair" punishment.
          Thus, if the algorithm gives a bad or biased punishment, the judge isn't held 
          responsible for the bad decision and can instead blame the algorithm,
        </li>
        <li> "Oh It's not my fault that punishment was so harsh, I'm just (blindly)
          following the algorithm"
        </li>
        <li>
          One notorious exmaple where judges put a signifigant amount of faith in the
           algorithm rather than relying on the judge's intuition can be found in a
           case in Wisconsin where a man was found guilty of stealing an lawnmower.
           The judge originally sentenced him to 1 year in jail, but after 
           hearing the opinion of COMPAS, decided to double the jail sentence to 2 years.
           A similar case was later brought to the Wisconsin Supreme Court (although 
           the court declined to hear the case) arguing that not being able to see
           how the algorithm had computed the defendant's risk-score (because
           COMPAS is a private closed-source software) was a violation of due process.
        </li>
      </ul>
      <li> <strike>Can't be bribed or threatened</strike> </li>
      <ul>
        <li>
          COMPAS is a closed source algorithm developed by Equivant.  Thus it is 
          immune to third-party discretion.  Thus it is impossible to determine
          what methods were used in its programming and if there could be any 
          intentional or unintentional problems within the program
        </li>
        <li>
          This refusal to reveal the source code of COMPAS was brought up in
          a court case Loomis v. Wisconsin, where they argued that not being able
          to see the innerworkings of COMPAS was a violation of due process.
        </li>
      </ul>
      <li> <strike>Consistent and Fair, Blind Justice</strike> </li>
      <ul>
        <li>
          Consistent and Fair justice is only our goal, if the justice is actually fair.
          In the case of COMPAS, ProPublica showed that COMPAS was racially biased, but
          since COMPAS doesn't know that it's "normal" is actually biased, putting
          faith in COMPAS to be Consistent and Fair when it was trained on biased data
          only perpetuates those unfarinesses and biases more, because now the biases
          are rooted into the algorithm as fair even though they are not
        </li>
        <li>
          To have a consistent and fair algorithm, you need an even starting ground.  Because
          COMPAS is biased, the initial starting ground isn't even, but COMPAS is pretending
          that it is which gives the illusion that COMPAS is fair when it is only normalizing
          the bias.
        </li>
      </ul>
    </ul>


    <p>
      Maybe you say I'm cherry picking my data, only showing the examples where white
      people were given low ratings and comparing them to the black people who had
      the highest ratings and then going "oh the humanity".  Fine, lets assume I am cherry
      picking my data and out in the results of the COMPAS algorithms there are white people
      who got disproportionaly longer sentences than their black equivalents. THEN THERE
      IS STILL A PROBLEM!!  It only proves my point more that the algorithms that are in use
      today in judicial systems are unreliable, dangerous, and can perpetuate sterotypes 
      and don't treat people fairly.  
    </p>
    <p>
      And you might argue, as long as its better than how human's are doing it then its
      good right??. NO it's not, because there is a distinct difference in how we view
      decisions made by humans vs decisions made by machines.  We all know, humans are falliable, 
      make mistakes, are biased, etc.  So when we see these disparities we think we can do better.
    </p>
    <p>
      Now think about machines.  When a machine gives you an answer, you almost always 
      assume that the answer the machine gave you is the correct and best answer.  We as 
      a society place a lot of faith in machines and algorithms, and by placing our trust
      in these COMPAS style algorithms we may be doing marginally better at giving fair
      sentences to people, but the fear is that we will settle with the fact
      that the machine is doing the best it absolutely can, and that any injustices or
      racisim the machine is percieved to have (in either direction) is just a result 
      of the "higher power" that is machine learning (see note on how no deep learning
      people really know how their algorithms work, it's pretty much impossible to determine
      how a deep learning model came to an answer, so we can't be sure if the machine 
      literally found a trend or if it is perpetuating racial biases that were fed into
      the machine at inception.)
    </p>
</div>

    <h1> Conclusion </h1>

    <h3> How do we fix This </h3>

    <p>
      <ul>
        <li> Algorithms that decide a defendants guilt should be open source in order to allow for easy scrutiny </li>
        
        <li>  By making the algorithm open source, that allows us to make sure that the algorithm is trained on Non-biased data. </li>
        
        <li> These algorithms should be used as a tool to guide us towards making better decisions,
        they should not be used in place of human interaction </li>
        <ul>
          <li> Think back to the Medical cancer detection, the algorithm on its own, had 8 false positives (very bad) </li>
          <li> Think back to the driving algorithm, it's good 99% of the time, but if the algorithms
            tells you to go continue down a street that is flooded, the human should jump into action
            and use it's better knowledge of the problem and not go down the flooded street </li>
        </ul>
        <li> Point is, the algorithms are a tool on your tool belt, not a magic black box solution to
        racial injustice </li>
        <li> These algorithms need to be scrutinized by anybody any time to ensure that
        they are done in the best way possible. </li>
      </ul>
    </p>
        


    <details>
      <summary>Details</summary>
      Something small enough to escape casual notice.
    </details>


    COMPAS softare, cite 12 wikipedia.
    https://en.wikipedia.org/wiki/COMPAS_(software)
    Another general criticism of machine-learning based algorithms is since they are data-dependent if the data are biased, the software will likely yield biased results.[12][page needed]

    Studies showing that COMPAS is no more accurate than individuals with little or no criminal justic experience.

    [1]: https://www.propublica.org/article/how-we-analyzed-the-compas-recidivism-algorithm




    <br>
    </br>
  </div>
</body>
