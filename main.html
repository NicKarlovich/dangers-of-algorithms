<head>
  <title>Algorithms Aren't The Solution</title>
  <style>
    .main {
      width: 60%;
      margin: 0 auto;
    }
    #title {
      width: 70%;
    }
  </style>
</head>
<body>
  <div class="main">
    <center>
    <h1 id="title">Algorithms Aren't The Solution To A 
      Broken Criminal Sentencing System, At Least Not Yet</h1>
      <div style="font-size: 20px;">
        <strong><p><i>By: Nick Karlovich</i></p></strong>
      </div>
    </center>
    <img width="1000px" height="800px" src=".\images\firstdraft-cover.png"/>
    <div>
      
        <p style="float:right;">
          <img src="https://external-content.duckduckgo.com/iu/?u=http%3A%2F%2Fmedia.philstar.com%2Fimages%2Fthe-philippine-star%2Fworld%2F20161220%2Fbrock-turner.jpg">
          A major problem within the U.S. Criminal Justice System is the inconsistency
          and unpredicatability of judges and their decisions on what the correct amount
          of punishment should be for any given crime.  There are a countless
          number of news stories showcasing how a criminal will get an extremely leniant
          punishment, then two days later a similar case results in the defendant recieving a 
          punishment that is so harsh in comparison to the previous case you have to wonder if the
          court system is just picking punishments at random.
          
          Some famous cases include: Brock Turner; who after being found guilty 
          (while pleading not guilty) on 3 accounts of sexual assault was given 6
           months in prison and 3 years of probation.  He was also released from prison
           3 months early.  This was seen by many as an incredible injustice and a 
           complete failing of the court system.
           
          Another famous case was of Guy Frank, who was sentenced to 20 years in jail
          for stealing 2 t-shirts, because under louisiana law, he was a repeat theft
          offender.  He was only recently released from prison with the help of the IPNO 
          (Innocense Project New Orleans).
          
          
          above could be drawn as: 
          
          | name  | name  | name  | name  |
          | pic   | pic   | pic   | pic   |
          | desc  | desc  | desc  | desc  |
          
          These are just two examples in a never ending list of sentencings that 
          show how inconsistent and unpredicatable the courts really are.
          
          It has actually been shown in studies how inconsistent and unpredicatable
          sentencings can be across different judges.
        
          In a study involving 47 virgina district attorneys, each judge was given 
          5 fabricated cases and were told to determine the guilt of the defendant
          and if found guilty, determine the appropriate sentence.
          
          Here are the results of 2 of their cases:
          
          Case A:
          Description:
          An eighteen year old female defendant who was apprehended for possession
of marijuana. She was arrested with her boyfriend and seven other acquaintances. Evidence
of a substantial amount of smoked and unsmoked marijuana was found; however, no
marijuana was discovered directly on the defendant's person. She had no previous criminal 
record, was a good student from a middle class home and was neither rebellious nor apologetic
for her actions.
          
          twenty-nine voted not guilty (61.7%)
          eighteen voted guilty (38.3%)
          - 8 (44.4%) probation
          - 4 (22.2%) fine
          - 3 (16.7%) probation & fine
          - 3 (16.7%) jail term
          
          so, depending on who you got as your judge you had a 
          61.7% chance of having a clean record, 
          17% chance of going on probation
          8.5% chance of getting a fine
          6.3% chance of getting probation and fine
          and wildly, there is a 
          6.3% chance of going to jail
          
          
          
          Case B:
          Description:
          This case was a burglary. Residents of a
household returned home to find their television set missing, and, hearing a noise, noticed
two persons carrying a large object down the
street. Soon afterwards, officers arrested a
forty-six year old man and his son, who were
.1 A more complete description of the cases is
pawning the TV set, which was identified by
its serial numbers. The father, whose fingerprints matched those found in the house, had
four previous convictions for petty larceny, one
for reckless driving and one for assaulting a
police officer. The son had only one traffic
conviction and claimed his father had told him
a friend gave him the television set.
          
          
          ================================
          In a more recent study 81 UK judges were asked to give bail to 
          41 fictious cases.  Of all these cases the judges failed to unanimously
          agree on any of the 41 cases presented before them.[hello world] An extra
          important note was that hidden amongst the 41 cases were 7 cases that
          were duplicates of earlier cases with only the names changed.
          
          It was found that most judges didn't manage to make the same decision 
          on their two identical cases.  Astonishingly, some judges did no better
          than matching their own answers than if they were, quite literally, awarding
          bail at random.[direct quote Hello World]
          

          In non simulated cases, other seemingly random events have been shown to
          have correlation with the outcomes of cases
          
          <<<[hello world, pg 75]
          Although why exactly this happens can't be pinned down, in her book Hello World, Hannah Fry 
          begins to describe how many factors play into how human judges make their
          decisions, "Judges with daughters are more likely to make decisions more favoriable to women[1], 
          judges are less likely to award bail if the local sports team has lost recently. [One] famous study 
          even suggested that the time of day affects your chances of a favorable outcome[6]...
          Another famous study showed that an individual judge will avoid making too many similar judgements
          in a row.  Your chances therefore of being award bail drop off a cliff if four successful cases
          were heard immediately before yours[64]"
          
          
          Given all this uncertainity and confusion in the court system, its no wonder
          we've begun to look for better solutions and in this modern age, computers 
          algorithms, machine learning, and AI are looked to as very enticing solutions.
          
          
          https://www.blackenterprise.com/louisiana-man-67-finally-released-from-prison-for-stealing-two-shirts-20-years-ago/
          
            (
            Proponents of racial equiality groups will typically point to specific cases
            where guilty white defendends will be given a lesser punishment for a crime,
            whereas a similar guilty black defendent will be given a harsher punishment
            for the same crime.  Personally, I do believe that this is an issue, but I
            am not making this argument here as it cannot be shown and is outside the scope 
            of this visualization.
          
            not saying it is statistically correlated that your race has an independent affect
            on the severity of a criminal punishment.  This fact is just very difficult to prove
            due to there being so many variables.  There are not enough cases where enough of these
            variables line up to draw any meaningful statistical conclusions (Judge, Race, Age,
            Previous Criminal History, Crime, Location, Time, Guilt, Remorse, Intent, etc.))
          
           A major problem within the U.S. Criminal Justice System is the existence
          of systematic bias in the sentencing of criminals of different races.  Now this can't
          be proved mathematically or with any true statistical signifigance.  
          
          Black
           individuals are more likely to be get longer prison sentences and be 
           considered "higher risk" compared to white individuals with similar criminal
            histories.  This was proven 
        </p>
    </div>
    <!-- News articles of people getting criminally long sentences when not needed -->
    <!-- <img src="images/black-vs-white-compas-scores.png">[1]</img>--> 
    <p>
      The exact reasoning behind why this is cannot be determined, but there are 
      possible reasons as to why human judges don't perform well when trying to fairly
      give punishments.
    </p>
    <ol type="1">
      <li>Biases</li>
      <ul>
        <li> Humans are known to be biased and it is often difficult for us
          to not let those influence our decisions [cite] </li>
      </ul>
      <li>Corruption</li>
      <ul>
        <li>Judges have the possibility to be bribed, convied, threatened into
           making decisions in favor of certain parties. 
            <p>
             [FOOTNOTE 1: it is also possible for algorithms to be modified, which
             is why a good algorithm would be publicly available to everybody (which 
             I suggest), then the argument could be made that you could compromise 
             the local integrity of a machine making these decisions, for which a
             counter argument could be made about all judicial decisions requiring
             a hash in order to validate results, for which a third counter argument
             could be made that you could give false hashes and you can probably see
             how this argument quickly spirals into a never ending circle of who to 
             trust which is too deep of a concept to cover in this visualization but
             I would suggest you check out [resource talking about security of
             download hashes] ]
             </p>
        </li>
      </ul>
      <li>Consistency & Fairness</li>
      <ul>
        <li>It has been shown that when judges are given the the same legal case
          that they will not consistently come to an agreement on what punishment
          is fair.  In this study [hello world] on a misdemeanor drug charge depending
          on what judge saw your case you had the chance to walk away with no jail time
          or could be put into jail for over 3 years, based purely on the judge's
          individual opinion
        </li>
        <li>Even more shockingly, judges don't make consistent decisions with themselves.
          In a study [hello world] done in ...., judges were given 60 cases, 7 of which
          were duplicates with the names changed and judges still did not give the cases
          the same sentencing, even though the details of the case were exactly the same.
        </li>
      </ul>
    </ol>
    <p>
      Given these glaring issues with judges it is no wonder why we've been looking 
      for more consistent alternatives to human judges.[footnote 2, I don't have
       anything against judges, they do their best and I appreciate that, it's only
       human nature that we're so inconsistent]  Recently, the U.S. Government
      has turned towards using algorithms as a possible solution.
    </p>    



    <p>
      So, before we get into algorithms for use in the courtroom. Lets take a look
      at how other fields of study have integrated algorithms into their work.
    </p>

    <h2> Algorithms for detecting breast cancer </h2>
    <p>
      PathAI: [3]https://www.pathai.com/what-we-do/

      An algorithm that assists a pathologist's cancer diagnosis process by reducing 
      the amount of time each diagnosis took.  The algorithm's introduction produced


      doctors are about 96% accurate in detecting cancers
      pg 88, (missed 27% of smaller hidden instances of cancer), 
      Never had a (false positive)

      best of algorithms from (CAMELBAK16) was 92.4% accurate, but with 8 false positives

      Combining these two together though, results in a 99.5% accuracy rating, with no false positives.
    </p>

    <h2> Intelligent Path Creation Algorithms </h2>

    <p> An algorithm that helps delivery truck drivers take more efficient routes when 
      delivering packages.

      Most notably these algorithms figured out important patterns to traffic in cities
      and have allowed companies like USPS/UPS/FedEx to deliver more packages quicker and 
      more efficiently.
    </p>

    <h3>
      <strong> So now that we've seen the kinds of ways that algorithms can be used to 
        solve problems that humans can't do well, lets put together what our <i>ideal</i>
        algorithm would have
      </strong>
    </h3>

<important>
  There is an important distinction between these algorithms, PATH AI and
   intelligent path creation. and the COMPAS algorithm.
   
   PathAI and intelligent path creation cannot run on its own.  They require 
   human interaction to run
   
   I fhte path AI went on its own, it would misdiagnose 8% of the people it scanned (not good, cite)
   
   if the intelligent path creation algorithm tried to run on its own, it
   would fail because it doesn't know the rules of the road, and couldn't react
   to rapidly changing road conditions.
   
   
   Compare this to the COMPAS algorithm.  It's new enough yet where the algorithm 
   isn't fully trusted on its own but that level of trust is rapidly approaching.
   
   The downsides to the other algorithms (people being misdiagnosed and car crashes)
   are much more visible to us than the downsides of the algorithm (racial biased
   steryotyping, etc.)  And why wouldn't judges want to use COMPAS to make their decisions
   for them.  Besides making their job easier, it gives the judges an easy get-out-of-jail free card
   if the algorithm misbehaves.  "Oh I didn't sentence that person to the death 
   penalty for jaywalking, it was the algorithm, it wasn't my fault.  The algorithm,
   is smarter than me so I trusted it..."

</important>









    <h3> Features of our Algorithm that can outperform humans </h3>

    <ul>
      <li> Unbiased </li>
      <li> More accurate </li>
      <li> Can't be bribed or threatened </li>
      <li> Consistent and Fair, Blind Justice </li>
    </ul>

    <h4> Goals: </h4>
    <ul>
      <li> Prevent future crimes </li>
      <li> Assign the minimal punishment that will prevent a criminal from re-offending </li>
    </ul>

    <h4> Not Goals: </h4>
    <ul>
      <li> Determine guilt of accused </li>
      <ul>
        <li> Our algorithm will assume that the person has been found guilty </li>
        <li> Determining guilt should be left up to judges and courts </li>
      </ul>
      <li> Minority Report / Person of Interest style of predicting crimes before they happen </li>
      <li> Stop first time offenders [note 3: argument could be made that punishments 
        by earlier court cases could be used as deterrent, but determining if certain 
        punishments influence future crimes is incredibly hard to determine (multivariate problems) 
        so for this visualization we won't be considering this possibility]
      </li>
    </ul>

    In order to better explain how our algorithm would work in theory the example of 
    Jim and Bill is given.

    Jim and his kid John are living below the poverty line and due to some unforseen
     medical expenses, Jim doesn't have enough money for groceries, so he goes to a 
     gas station and steals some rice and bread.  Jim normally wouldn't do this but
      given the option between starving his child and stealing, he's going to take 
      his chances.
      
    Bill isn't in an ideal place economically, he has a mortgage and a car loan etc, but
    he can definitely pay for his groceries and doesn't need to worry about going hungry.
    Bill steals a candy bar and soda from the same gas station as Jim because he just
    doesn't feel like spending the $3 to buy them.

    Jim and Bill both get caught and are found guilty of stealing.  Here is where our
    algorithm comes in and decides how much of what kind of punishment each person
    should get in order to disuade them from ever commiting another crime.
    [Note 4: Special note here, care is needed to MINIMIZE the time, without this
    clause, an algorithm might quickly learn that if for any crime committed, the optimal
     punishment is death.  The algorithm would have successfully created a solution
     where there is a 0% reoffend rate... at the cost of murdering every single
     person who ever commits a crime.]

    <img src="./images/basic-reoffend-table.png"/>

    The optimal solution to the cost-matrix above is for our algorithm to put Bill away for X 
    months and for Jim to get connected with some food-stamp / free meals program as 
    this will minimize the reoffend chances for both people.

    Obviously this example is very Black & White and the real world isn't as cut and dry 
    as this, but figuring that out should be the job of a large, complex, algorithm 
    that can detect patterns better than us humans can.

    So, this algorithm seems pretty great right?
    Well, if only this is how it actually worked.  Instead we have COMPAS.


    COMPAS is a "risk-assessment" algorithm developed by Equivant and is used by some U.S. courts
    to assess the likelyhood of a defendant re-offending.

    COMPAS has been under fire since its inception for its racially biased results (pro publica
    article here).  Although there has been counter research re-affirming that an algorithm
    can be created to show actuarial risk can be predicted free of racial and/or gender bias"
    (although this doesn't imply that COMPAS is one of those algorithms that does this.)

    Futher reserach has brought up that COMPAS software is only slightly more accurate than
    individuals with little or no criminal justice experience and is less accurate than
    when those individuals answers are pooled together (63% for individual, 65% for COMPAS, 
    67% for individuals grouped.)  Which aren't particularly inspiring statistics for
    an algorithm which is supposed [to replace judges in sentencing](I might be straw
    maning the opponent here, need to find a source for this)

    Looking at COMPAS and comparing it to the features that we looked for before,
    we can actually see how those features that we assumed earlier have actually
    come to make the algorithms we have now WORSE.

    <ul>
      <li> <strike>Unbiased</strike> </li>
      <ul>
        <li>
          This is only true if the training data that is provided to the algorithm is
          unbiased as well, and because COMPAS is not open-source, we don't know what
          data they used to train their algorithm (although it is likely that if they
          used previous court cases as their data, that the biases and racial disparities
          that exist in our system today was fed into the algorithm as the "norm")
        </li>
      </ul>
      <li> <strike>More accurate</strike> </li>
      <ul>
        <li>
          A "more accurate" algorithm allows for judges to fully rely on the algorithm
          even if the algorithm gives what would typically be considered an "unfair" punishment.
          Thus, if the algorithm gives a bad or biased punishment, the judge isn't held 
          responsible for the bad decision and can instead blame the algorithm,
        </li>
        <li> "Oh It's not my fault that punishment was so harsh, I'm just (blindly)
          following the algorithm"
        </li>
        <li>
          One notorious exmaple where judges put a signifigant amount of faith in the
           algorithm rather than relying on the judge's intuition can be found in a
           case in Wisconsin where a man was found guilty of stealing an lawnmower.
           The judge originally sentenced him to 1 year in jail, but after 
           hearing the opinion of COMPAS, decided to double the jail sentence to 2 years.
           A similar case was later brought to the Wisconsin Supreme Court (although 
           the court declined to hear the case) arguing that not being able to see
           how the algorithm had computed the defendant's risk-score (because
           COMPAS is a private closed-source software) was a violation of due process.
        </li>
      </ul>
      <li> <strike>Can't be bribed or threatened</strike> </li>
      <ul>
        <li>
          COMPAS is a closed source algorithm developed by Equivant.  Thus it is 
          immune to third-party discretion.  Thus it is impossible to determine
          what methods were used in its programming and if there could be any 
          intentional or unintentional problems within the program
        </li>
        <li>
          This refusal to reveal the source code of COMPAS was brought up in
          a court case Loomis v. Wisconsin, where they argued that not being able
          to see the innerworkings of COMPAS was a violation of due process.
        </li>
      </ul>
      <li> <strike>Consistent and Fair, Blind Justice</strike> </li>
      <ul>
        <li>
          Consistent and Fair justice is only our goal, if the justice is actually fair.
          In the case of COMPAS, ProPublica showed that COMPAS was racially biased, but
          since COMPAS doesn't know that it's "normal" is actually biased, putting
          faith in COMPAS to be Consistent and Fair when it was trained on biased data
          only perpetuates those unfarinesses and biases more, because now the biases
          are rooted into the algorithm as fair even though they are not
        </li>
        <li>
          To have a consistent and fair algorithm, you need an even starting ground.  Because
          COMPAS is biased, the initial starting ground isn't even, but COMPAS is pretending
          that it is which gives the illusion that COMPAS is fair when it is only normalizing
          the bias.
        </li>
      </ul>
    </ul>


    <p>
      Maybe you say I'm cherry picking my data, only showing the examples where white
      people were given low ratings and comparing them to the black people who had
      the highest ratings and then going "oh the humanity".  Fine, lets assume I am cherry
      picking my data and out in the results of the COMPAS algorithms there are white people
      who got disproportionaly longer sentences than their black equivalents. THEN THERE
      IS STILL A PROBLEM!!  It only proves my point more that the algorithms that are in use
      today in judicial systems are unreliable, dangerous, and can perpetuate sterotypes 
      and don't treat people fairly.  
    </p>
    <p>
      And you might argue, as long as its better than how human's are doing it then its
      good right??. NO it's not, because there is a distinct difference in how we view
      decisions made by humans vs decisions made by machines.  We all know, humans are falliable, 
      make mistakes, are biased, etc.  So when we see these disparities we think we can do better.
    </p>
    <p>
      Now think about machines.  When a machine gives you an answer, you almost always 
      assume that the answer the machine gave you is the correct and best answer.  We as 
      a society place a lot of faith in machines and algorithms, and by placing our trust
      in these COMPAS style algorithms we may be doing marginally better at giving fair
      sentences to people, but the fear is that we will settle with the fact
      that the machine is doing the best it absolutely can, and that any injustices or
      racisim the machine is percieved to have (in either direction) is just a result 
      of the "higher power" that is machine learning (see note on how no deep learning
      people really know how their algorithms work, it's pretty much impossible to determine
      how a deep learning model came to an answer, so we can't be sure if the machine 
      literally found a trend or if it is perpetuating racial biases that were fed into
      the machine at inception.)
    </p>


    <h1> Conclusion </h1>

    <h3> How do we fix This </h3>

    <p>
      <ul>
        <li> Algorithms that decide a defendants guilt should be open source in order to allow for easy scrutiny </li>
        
        <li>  By making the algorithm open source, that allows us to make sure that the algorithm is trained on Non-biased data. </li>
        
        <li> These algorithms should be used as a tool to guide us towards making better decisions,
        they should not be used in place of human interaction </li>
        <ul>
          <li> Think back to the Medical cancer detection, the algorithm on its own, had 8 false positives (very bad) </li>
          <li> Think back to the driving algorithm, it's good 99% of the time, but if the algorithms
            tells you to go continue down a street that is flooded, the human should jump into action
            and use it's better knowledge of the problem and not go down the flooded street </li>
        </ul>
        <li> Point is, the algorithms are a tool on your tool belt, not a magic black box solution to
        racial injustice </li>
        <li> These algorithms need to be scrutinized by anybody any time to ensure that
        they are done in the best way possible. </li>
      </ul>
    </p>
        


    <details>
      <summary>Details</summary>
      Something small enough to escape casual notice.
    </details>


    COMPAS softare, cite 12 wikipedia.
    https://en.wikipedia.org/wiki/COMPAS_(software)
    Another general criticism of machine-learning based algorithms is since they are data-dependent if the data are biased, the software will likely yield biased results.[12][page needed]

    Studies showing that COMPAS is no more accurate than individuals with little or no criminal justic experience.

    [1]: https://www.propublica.org/article/how-we-analyzed-the-compas-recidivism-algorithm




    <br>
    </br>
  </div>
</body>
